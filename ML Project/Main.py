# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12kP_8OCJ-pBHFeeSdCDpoJEuA29OSsWd
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import chardet
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import TextVectorization
# from google.colab import files
# files.upload()


def compile_model(model):
    '''
    simply compile the model with adam optimzer
    '''
    model.compile(optimizer=keras.optimizers.Adam(),
                  loss=keras.losses.BinaryCrossentropy(),
                  metrics=['accuracy'])


def fit_model(model, epochs, x_train=x_train, y_train=y_train,
              x_test=x_test, y_test=y_test):
    '''
    fit the model with given epochs, train
    and test data
    '''
    history = model.fit(x_train,
                        y_train,
                        epochs=epochs,
                        validation_data=(x_test, y_test),
                        validation_steps=int(0.2*len(x_test)))
    return history


def evaluate_model(model, X, y):
    '''
    evaluate the model and returns accuracy,
    precision, recall and f1-score
    '''
    y_preds = np.round(model.predict(X))
    accuracy = accuracy_score(y, y_preds)
    precision = precision_score(y, y_preds)
    recall = recall_score(y, y_preds)
    f1 = f1_score(y, y_preds)

    model_results_dict = {'accuracy': accuracy,
                          'precision': precision,
                          'recall': recall,
                          'f1-score': f1}

    return model_results_dict


if __name__ == '__main__':

    with open("spam.csv", "rb") as rawdata:
        result = chardet.detect(rawdata.read(10000))

    df = pd.read_csv("spam.csv", encoding=result['encoding'])
    df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)
    df = df.rename(columns={'v1': 'label', 'v2': 'message'})
    df['label_encoding'] = df['label'].map({'ham': 0, 'spam': 1})
    # print(df)

    df = pd.read_csv("spam.csv", encoding=result['encoding'])
    df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)
    df = df.rename(columns={'v1': 'label', 'v2': 'message'})
    df['label_encoding'] = df['label'].map({'ham': 0, 'spam': 1})
    print(df)

    x, y = np.asanyarray(df['message']), np.asanyarray(df['label_encoding'])

    new_df = pd.DataFrame({'message': x, 'label': y})
    print(new_df)

    x_train, x_test, y_train, y_test = train_test_split(
        new_df['message'], new_df['label'], test_size=0.2, random_state=42)

    tfidf_vec = TfidfVectorizer().fit(x_train)
    x_train_vec, x_test_vec = tfidf_vec.transform(
        x_train), tfidf_vec.transform(x_test)

    baseline_model = MultinomialNB()
    baseline_model.fit(x_train_vec, y_train)

    bm_accuracy = accuracy_score(y_test, baseline_model.predict(x_test_vec))

    print(bm_accuracy)

    print(classification_report(y_test, baseline_model.predict(x_test_vec)))

    avg_words_len = round(sum([len(i.split())
                          for i in df['message']])/len(df['message']))
    print(avg_words_len)

    s = set()
    for sent in df['message']:
    for word in sent.split():
        s.add(word)
    total_words_length = len(s)
    print(total_words_length)

    MAXTOKENS = total_words_length
    OUTPUTLEN = avg_words_len

    text_vec = TextVectorization(
        max_tokens=MAXTOKENS,
        standardize='lower_and_strip_punctuation',
        output_mode='int',
        output_sequence_length=OUTPUTLEN
    )
    text_vec.adapt(x_train)

    embedding_layer = layers.Embedding(
        input_dim=MAXTOKENS,
        output_dim=128,
        embeddings_initializer='uniform',
        input_length=OUTPUTLEN
    )

    input_layer = layers.Input(shape=(1,), dtype=tf.string)
    vec_layer = text_vec(input_layer)
    embedding_layer_model = embedding_layer(vec_layer)
    l = layers.GlobalAveragePooling1D()(embedding_layer_model)
    l = layers.Flatten()(l)
    l = layers.Dense(32, activation='relu')(l)
    output_layer = layers.Dense(1, activation='sigmoid')(l)
    model_1 = keras.Model(input_layer, output_layer)

    model_1.compile(optimizer='adam', loss=keras.losses.BinaryCrossentropy(
        label_smoothing=0.5), metrics=['accuracy'])

    model_1.summary()

    history_1 = model_1.fit(x_train, y_train, epochs=5, validation_data=(
        x_test, y_test), validation_steps=int(0.2*len(x_test)))

    pd.DataFrame(history_1.history).plot()

    df_created = pd.DataFrame(history_1.history)

    print(df_created)

    model_1.summary()

    import tensorflow_hub as hub

    model_3 = keras.Sequential()

    # universal-sentence-encoder layer
    # directly from tfhub
    use_layer = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder/4",
                               trainable=False,
                               input_shape=[],
                               dtype=tf.string,
                               name='USE')
    model_3.add(use_layer)
    model_3.add(layers.Dropout(0.2))
    model_3.add(layers.Dense(64, activation=keras.activations.relu))
    model_3.add(layers.Dense(1, activation=keras.activations.sigmoid))

    compile_model(model_3)

    history_3 = fit_model(model_3, epochs=5)

    model_3 = keras.Sequential()

    # universal-sentence-encoder layer
    # directly from tfhub
    use_layer = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder/4",
                               trainable=False,
                               input_shape=[],
                               dtype=tf.string,
                               name='USE')
    model_3.add(use_layer)
    model_3.add(layers.Dropout(0.2))
    model_3.add(layers.Dense(64, activation=keras.activations.relu))
    model_3.add(layers.Dense(1, activation=keras.activations.sigmoid))

    compile_model(model_3)

    history_3 = fit_model(model_3, epochs=5)

    input_layer = layers.Input(shape=(1,), dtype=tf.string)
    vec_layer = text_vec(input_layer)
    embedding_layer_model = embedding_layer(vec_layer)
    bi_lstm = layers.Bidirectional(layers.LSTM(
        64, activation='tanh', return_sequences=True))(embedding_layer_model)
    lstm = layers.Bidirectional(layers.LSTM(64))(bi_lstm)
    flatten = layers.Flatten()(lstm)
    dropout = layers.Dropout(.1)(flatten)
    l2 = layers.Dense(32, activation='relu')(dropout)
    output_layer = layers.Dense(1, activation='sigmoid')(l2)
    model_2 = keras.Model(input_layer, output_layer)

    compile_model(model_2)  # compile the model
    history_2 = fit_model(model_2, epochs=5)  # fit the model

    model_1.summary()

    model_1.summary()
    model_2.summary()
    model_3.summary()

    pd.DataFrame(history_1.history).plot()
    pd.DataFrame(history_2.history).plot()
    pd.DataFrame(history_3.history).plot()

    pd.DataFrame(history_1.history)

    pd.DataFrame(history_2.history)

    pd.DataFrame(history_3.history)
